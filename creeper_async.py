#!/usr/bin/env python3
"""
Creeper V1.0 - ç½‘é¡µçˆ¬è™«å·¥å…·(å¼‚æ­¥å¹¶å‘ç‰ˆæœ¬)
å°† Markdown æ–‡ä»¶ä¸­çš„ URL æ‰¹é‡çˆ¬å–å¹¶ä¿å­˜ä¸ºç»“æ„åŒ–çš„æœ¬åœ° Markdown æ–‡æ¡£
"""

import sys
import argparse
import asyncio
from pathlib import Path
from tqdm.asyncio import tqdm as async_tqdm

from src.parser import MarkdownParser
from src.dedup import DedupManager
from src.async_fetcher import AsyncWebFetcher
from src.cookie_manager import CookieManager
from src.storage import StorageManager
from src.config import config
from src.utils import setup_logger

logger = setup_logger("creeper")


class AsyncCreeper:
    """Creeper å¼‚æ­¥ä¸»ç±»"""

    def __init__(self, args):
        """
        åˆå§‹åŒ– Creeper

        Args:
            args: å‘½ä»¤è¡Œå‚æ•°
        """
        self.args = args
        self.stats = {
            'total': 0,
            'success': 0,
            'skipped': 0,
            'failed': 0
        }
        self.failed_items = []

        # åˆå§‹åŒ– Cookie ç®¡ç†å™¨(å¦‚æœæŒ‡å®š)
        self.cookie_manager = None
        if args.cookies_file:
            # ä½¿ç”¨æ–‡ä»¶å­˜å‚¨æ¨¡å¼(å‘åå…¼å®¹)
            self.cookie_manager = CookieManager(
                cookies_file=args.cookies_file,
                format='json',
                storage_backend='file'
            )
            logger.info(f"å·²å¯ç”¨ Cookie ç®¡ç†(æ–‡ä»¶æ¨¡å¼),æ–‡ä»¶: {args.cookies_file}")
        elif config.COOKIE_STORAGE == 'redis':
            # ä½¿ç”¨ Redis å­˜å‚¨æ¨¡å¼
            self.cookie_manager = CookieManager(
                storage_backend='redis',
                redis_client=None,  # å»¶è¿Ÿåˆå§‹åŒ–
                redis_key_prefix=config.COOKIE_REDIS_KEY_PREFIX,
                expire_days=config.COOKIE_EXPIRE_DAYS
            )
            logger.info(f"å·²å¯ç”¨ Cookie ç®¡ç†(Redis æ¨¡å¼),è¿‡æœŸæ—¶é—´: {config.COOKIE_EXPIRE_DAYS} å¤©")

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.parser = None
        self.dedup = DedupManager()

        # å¦‚æœ cookie_manager ä½¿ç”¨ Redis æ¨¡å¼,ä¼ å…¥ redis_client
        if self.cookie_manager and self.cookie_manager.storage_backend == 'redis':
            self.cookie_manager.redis_client = self.dedup.redis
            # é‡æ–°åŠ è½½ Cookie
            self.cookie_manager._load_all_from_redis()

        self.fetcher = AsyncWebFetcher(
            use_playwright=not args.no_playwright,
            concurrency=args.concurrency,
            cookie_manager=self.cookie_manager
        )
        self.storage = StorageManager(args.output)

    async def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            logger.info("=" * 60)
            logger.info(f"Creeper v1.0.0 - å¼‚æ­¥å¹¶å‘æ¨¡å¼")
            logger.info("=" * 60)

            # 1. è§£æ Markdown æ–‡ä»¶
            logger.info(f"æ­£åœ¨è§£ææ–‡ä»¶: {self.args.input_file}")
            self.parser = MarkdownParser(self.args.input_file)
            items = self.parser.parse()

            if not items:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½• URL,ç¨‹åºé€€å‡º")
                return

            self.stats['total'] = len(items)
            logger.info(f"å…±æ‰¾åˆ° {self.stats['total']} ä¸ª URL")
            logger.info(f"å¹¶å‘æ•°: {self.fetcher.concurrency}")

            # æ˜¾ç¤ºæ–‡æ¡£ç»“æ„(å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼)
            if config.DEBUG:
                self.parser.display_structure()

            # 2. æµ‹è¯• Redis è¿æ¥
            if not self.dedup.test_connection():
                logger.warning("Redis è¿æ¥å¤±è´¥,å°†è·³è¿‡å»é‡æ£€æŸ¥")

            # 3. å¼‚æ­¥å¤„ç†æ¯ä¸ª URL
            logger.info("å¼€å§‹çˆ¬å–ç½‘é¡µ(å¼‚æ­¥å¹¶å‘)...")
            tasks = []
            for item in items:
                task = self._process_url(item)
                tasks.append(task)

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            for coro in async_tqdm.as_completed(tasks, desc="çˆ¬å–è¿›åº¦", unit="url", total=len(tasks)):
                await coro

            # 4. ä¿å­˜å¤±è´¥çš„ URL
            if self.failed_items:
                self.storage.save_failed_urls(self.failed_items)

            # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self._display_stats()

            logger.info("=" * 60)
            logger.info("çˆ¬å–ä»»åŠ¡å®Œæˆ!")
            logger.info("=" * 60)

        except KeyboardInterrupt:
            logger.warning("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
            self._display_stats()
            sys.exit(1)
        except Exception as e:
            logger.error(f"ç¨‹åºå¼‚å¸¸: {e}", exc_info=config.DEBUG)
            sys.exit(1)
        finally:
            # ä¿å­˜ Cookie(å¦‚æœæœ‰)
            if self.cookie_manager and self.args.save_cookies:
                if self.cookie_manager.save():
                    logger.info(f"Cookie å·²ä¿å­˜")

            # æ¸…ç†èµ„æº
            self.dedup.close()

    async def _process_url(self, item):
        """
        å¼‚æ­¥å¤„ç†å•ä¸ª URL

        Args:
            item: URLItem å¯¹è±¡
        """
        url = item.url

        try:
            # å»é‡æ£€æŸ¥
            if not self.args.force and self.dedup.is_crawled(url):
                logger.info(f"âŠ˜ è·³è¿‡(å·²çˆ¬å–): {url}")
                self.stats['skipped'] += 1
                return

            # å¼‚æ­¥çˆ¬å–ç½‘é¡µ
            page = await self.fetcher.fetch(url)

            if not page.success:
                logger.error(f"âœ— çˆ¬å–å¤±è´¥: {url} - {page.error}")
                self.stats['failed'] += 1
                self.failed_items.append((item, page.error or "æœªçŸ¥é”™è¯¯"))
                return

            # ä¿å­˜æ–‡ä»¶(åŒæ­¥æ“ä½œ,ä½†å¾ˆå¿«)
            file_path = self.storage.save(item, page)

            if file_path:
                # æ ‡è®°ä¸ºå·²çˆ¬å–
                self.dedup.mark_crawled(url)
                self.stats['success'] += 1
                logger.info(f"âœ“ æˆåŠŸ: {url}")
            else:
                self.stats['failed'] += 1
                self.failed_items.append((item, "ä¿å­˜æ–‡ä»¶å¤±è´¥"))
                logger.error(f"âœ— ä¿å­˜å¤±è´¥: {url}")

        except Exception as e:
            logger.error(f"âœ— å¤„ç†å¼‚å¸¸: {url} - {e}")
            self.stats['failed'] += 1
            self.failed_items.append((item, str(e)))

    def _display_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è®¡:   {self.stats['total']} ä¸ª URL")
        print(f"æˆåŠŸ:   {self.stats['success']} ä¸ª âœ“")
        print(f"è·³è¿‡:   {self.stats['skipped']} ä¸ª âŠ˜")
        print(f"å¤±è´¥:   {self.stats['failed']} ä¸ª âœ—")

        if self.stats['total'] > 0:
            success_rate = (self.stats['success'] / self.stats['total']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")

        print("=" * 60)

        # æ˜¾ç¤ºè¾“å‡ºç›®å½•
        storage_stats = self.storage.get_stats()
        print(f"\nè¾“å‡ºç›®å½•: {storage_stats['output_dir']}")
        print(f"ç”Ÿæˆæ–‡ä»¶: {storage_stats['total_files']} ä¸ª")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='Creeper V1.0 - ç½‘é¡µçˆ¬è™«å·¥å…·(å¼‚æ­¥å¹¶å‘ç‰ˆæœ¬)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s input.md                    # åŸºæœ¬ä½¿ç”¨
  %(prog)s input.md -o ./output        # æŒ‡å®šè¾“å‡ºç›®å½•
  %(prog)s input.md -c 10              # è®¾ç½®å¹¶å‘æ•°ä¸º 10
  %(prog)s input.md --debug            # å¼€å¯è°ƒè¯•æ¨¡å¼
  %(prog)s input.md --force            # å¼ºåˆ¶é‡æ–°çˆ¬å–
  %(prog)s input.md --no-playwright    # ç¦ç”¨ Playwright

æ›´å¤šä¿¡æ¯: https://github.com/your-repo/creeper
        """
    )

    parser.add_argument(
        'input_file',
        type=str,
        nargs='?',  # å¯é€‰å‚æ•°
        default=None,
        help='Markdown è¾“å…¥æ–‡ä»¶è·¯å¾„'
    )

    parser.add_argument(
        '-o', '--output',
        type=str,
        default=config.OUTPUT_DIR,
        help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {config.OUTPUT_DIR})'
    )

    parser.add_argument(
        '-c', '--concurrency',
        type=int,
        default=config.CONCURRENCY,
        help=f'å¹¶å‘æ•° (é»˜è®¤: {config.CONCURRENCY})'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°çˆ¬å–(è·³è¿‡å»é‡æ£€æŸ¥)'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='å¼€å¯è°ƒè¯•æ¨¡å¼'
    )

    parser.add_argument(
        '--no-playwright',
        action='store_true',
        help='ç¦ç”¨ Playwright(ä»…ä½¿ç”¨é™æ€çˆ¬å–)'
    )

    parser.add_argument(
        '--cookies-file',
        type=str,
        default=None,
        help='Cookie å­˜å‚¨æ–‡ä»¶è·¯å¾„(å¯ç”¨ Cookie ç®¡ç†)'
    )

    parser.add_argument(
        '--save-cookies',
        action='store_true',
        help='çˆ¬å–ç»“æŸåä¿å­˜ Cookie'
    )

    parser.add_argument(
        '--login-url',
        type=str,
        default=None,
        help='éœ€è¦ç™»å½•çš„ URL,å¯åŠ¨äº¤äº’å¼ç™»å½•æµç¨‹'
    )

    parser.add_argument(
        '-v', '--version',
        action='version',
        version='Creeper 1.2.0'
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()

    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    if args.debug:
        config.DEBUG = True
        config.LOG_LEVEL = 'DEBUG'
        # é‡æ–°è®¾ç½® logger
        import logging
        logging.getLogger("creeper").setLevel(logging.DEBUG)

    # å¦‚æœæŒ‡å®šäº† --login-url,æ‰§è¡Œäº¤äº’å¼ç™»å½•
    if args.login_url:
        from src.interactive_login import interactive_login

        async def do_login():
            try:
                # æ‰§è¡Œäº¤äº’å¼ç™»å½•
                domain_cookies = await interactive_login(
                    args.login_url,
                    timeout=config.INTERACTIVE_LOGIN_TIMEOUT
                )

                if not domain_cookies:
                    logger.error("æœªæå–åˆ°ä»»ä½• Cookie")
                    return False

                # åˆ›å»º cookie_manager
                from src.dedup import DedupManager
                dedup = DedupManager()

                cookie_manager = CookieManager(
                    storage_backend='redis',
                    redis_client=dedup.redis,
                    redis_key_prefix=config.COOKIE_REDIS_KEY_PREFIX,
                    expire_days=config.COOKIE_EXPIRE_DAYS
                )

                # ä¿å­˜ Cookie
                for domain, cookies in domain_cookies.items():
                    cookie_manager.set_cookies(domain, cookies)

                success = cookie_manager.save()

                if success:
                    logger.info("=" * 60)
                    logger.info("âœ… Cookie å·²æˆåŠŸä¿å­˜åˆ° Redis")
                    logger.info(f"   å…±ä¿å­˜ {len(domain_cookies)} ä¸ªåŸŸçš„ Cookie")
                    logger.info(f"   è¿‡æœŸæ—¶é—´: {config.COOKIE_EXPIRE_DAYS} å¤©")
                    logger.info("   åç»­çˆ¬å–å°†è‡ªåŠ¨ä½¿ç”¨è¿™äº› Cookie")
                    logger.info("=" * 60)
                else:
                    logger.error("Cookie ä¿å­˜å¤±è´¥")
                    return False

                dedup.close()
                return True

            except Exception as e:
                logger.error(f"äº¤äº’å¼ç™»å½•å¤±è´¥: {e}", exc_info=True)
                return False

        # è¿è¡Œäº¤äº’å¼ç™»å½•
        success = asyncio.run(do_login())
        sys.exit(0 if success else 1)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.input_file:
        logger.error("é”™è¯¯: å¿…é¡»æä¾›è¾“å…¥æ–‡ä»¶æˆ–ä½¿ç”¨ --login-url è¿›è¡Œç™»å½•")
        sys.exit(1)

    if not Path(args.input_file).exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        sys.exit(1)

    # è¿è¡Œå¼‚æ­¥çˆ¬è™«
    creeper = AsyncCreeper(args)
    asyncio.run(creeper.run())


if __name__ == '__main__':
    main()
