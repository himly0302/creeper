"""
æ–‡ä»¶å­˜å‚¨æ¨¡å—
ç”Ÿæˆç›®å½•ç»“æ„å¹¶ä¿å­˜ Markdown æ–‡ä»¶
"""

import asyncio
from pathlib import Path
from typing import Optional

from .fetcher import WebPage
from .parser import URLItem
from .cleaner import ContentCleaner
from .config import config
from .utils import setup_logger, sanitize_filename, ensure_dir, get_timestamp
from .image_downloader import ImageDownloader, AsyncImageDownloader

logger = setup_logger(__name__)


class StorageManager:
    """æ–‡ä»¶å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, output_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•,å¦‚æœä¸º None åˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        self.output_dir = Path(output_dir or config.OUTPUT_DIR)
        ensure_dir(self.output_dir)
        logger.info(f"æ–‡ä»¶å­˜å‚¨ç®¡ç†å™¨å·²åˆå§‹åŒ–: {self.output_dir}")

    def save(self, item: URLItem, page: WebPage) -> Optional[Path]:
        """
        ä¿å­˜ç½‘é¡µä¸º Markdown æ–‡ä»¶

        Args:
            item: URL é¡¹ç›®(åŒ…å« H1/H2 å±‚çº§ä¿¡æ¯)
            page: ç½‘é¡µæ•°æ®

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„,å¤±è´¥è¿”å› None
        """
        try:
            # æ„å»ºç›®å½•è·¯å¾„: output_dir/H1/H2/
            h1_dir = self.output_dir / sanitize_filename(item.h1)
            h2_dir = h1_dir / sanitize_filename(item.h2)
            ensure_dir(h2_dir)

            # æ„å»ºæ–‡ä»¶å: æ ‡é¢˜.md
            filename = sanitize_filename(page.title) + ".md"
            file_path = h2_dir / filename

            # ç”Ÿæˆ Markdown å†…å®¹
            markdown_content = self._generate_markdown(item, page, h2_dir)

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            logger.info(f"âœ“ æ–‡ä»¶å·²ä¿å­˜: {file_path.relative_to(self.output_dir)}")
            return file_path

        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def _generate_markdown(self, item: URLItem, page: WebPage, h2_dir: Path) -> str:
        """
        ç”Ÿæˆ Markdown æ–‡ä»¶å†…å®¹

        Args:
            item: URL é¡¹ç›®
            page: ç½‘é¡µæ•°æ®
            h2_dir: H2 çº§ç›®å½•è·¯å¾„ï¼ˆç”¨äºä¿å­˜å›¾ç‰‡ï¼‰

        Returns:
            Markdown æ ¼å¼çš„æ–‡ä»¶å†…å®¹
        """
        # æ¸…æ´—å†…å®¹
        content = ContentCleaner.clean(page.content)
        description = ContentCleaner.truncate_description(page.description, 300)

        # æ™ºèƒ½å›¾ç‰‡ä¸‹è½½å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.DOWNLOAD_IMAGES:
            try:
                logger.debug("å›¾ç‰‡ä¸‹è½½åŠŸèƒ½å·²å¯ç”¨ï¼Œå¼€å§‹æ™ºèƒ½å¤„ç†å›¾ç‰‡...")
                downloader = ImageDownloader(base_url=page.url)
                images_dir = h2_dir / "images"

                # ä»æ¸…æ´—åçš„å†…å®¹ä¸­æå–å›¾ç‰‡ URL
                markdown_images = downloader.extract_markdown_images(content)

                if markdown_images:
                    logger.info(f"ä»æ¸…æ´—åçš„å†…å®¹ä¸­å‘ç° {len(markdown_images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
                    # åªä¸‹è½½åœ¨æ¸…æ´—åå†…å®¹ä¸­å­˜åœ¨çš„å›¾ç‰‡
                    content = downloader.download_valid_images(content, markdown_images, images_dir)
                else:
                    logger.debug("æ¸…æ´—åçš„å†…å®¹ä¸­æ²¡æœ‰å‘ç°å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½")

                downloader.close()
            except Exception as e:
                logger.warning(f"âš  æ™ºèƒ½å›¾ç‰‡ä¸‹è½½å¤„ç†å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹å†…å®¹: {e}")

    async def _generate_markdown_async(self, item: URLItem, page: WebPage, h2_dir: Path) -> str:
        """
        å¼‚æ­¥ç”Ÿæˆ Markdown æ–‡ä»¶å†…å®¹

        Args:
            item: URL é¡¹ç›®
            page: ç½‘é¡µæ•°æ®
            h2_dir: H2 çº§ç›®å½•è·¯å¾„ï¼ˆç”¨äºä¿å­˜å›¾ç‰‡ï¼‰

        Returns:
            Markdown æ ¼å¼çš„æ–‡ä»¶å†…å®¹
        """
        # æ¸…æ´—å†…å®¹
        content = ContentCleaner.clean(page.content)
        description = ContentCleaner.truncate_description(page.description, 300)

        # æ™ºèƒ½å¼‚æ­¥å›¾ç‰‡ä¸‹è½½å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.DOWNLOAD_IMAGES:
            try:
                logger.debug("å›¾ç‰‡ä¸‹è½½åŠŸèƒ½å·²å¯ç”¨ï¼Œå¼€å§‹å¼‚æ­¥æ™ºèƒ½å¤„ç†å›¾ç‰‡...")
                downloader = AsyncImageDownloader(base_url=page.url)
                images_dir = h2_dir / "images"

                # ä»æ¸…æ´—åçš„å†…å®¹ä¸­æå–å›¾ç‰‡ URL
                markdown_images = downloader.extract_markdown_images(content)

                if markdown_images:
                    logger.info(f"ä»æ¸…æ´—åçš„å†…å®¹ä¸­å‘ç° {len(markdown_images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å¼‚æ­¥ä¸‹è½½...")
                    # å¼‚æ­¥ä¸‹è½½åœ¨æ¸…æ´—åå†…å®¹ä¸­å­˜åœ¨çš„å›¾ç‰‡
                    content = await downloader.download_valid_images(content, markdown_images, images_dir)
                else:
                    logger.debug("æ¸…æ´—åçš„å†…å®¹ä¸­æ²¡æœ‰å‘ç°å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½")

            except Exception as e:
                logger.warning(f"âš  æ™ºèƒ½å¼‚æ­¥å›¾ç‰‡ä¸‹è½½å¤„ç†å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹å†…å®¹: {e}")

        # æ„å»º Markdown
        lines = []

        # æ ‡é¢˜
        lines.append(f"# {page.title}")
        lines.append("")

        # å…ƒä¿¡æ¯
        lines.append(f"> ğŸ“… **çˆ¬å–æ—¶é—´**: {page.crawled_at}")
        lines.append(f"> ğŸ”— **æ¥æºé“¾æ¥**: {page.url}")

        if description:
            lines.append(f"> ğŸ“ **ç½‘é¡µæè¿°**: {description}")

        if page.author:
            lines.append(f"> âœï¸ **ä½œè€…**: {page.author}")

        if page.published_date:
            lines.append(f"> ğŸ“† **å‘å¸ƒæ—¶é—´**: {page.published_date}")

        lines.append(f"> ğŸ¯ **çˆ¬å–æ–¹å¼**: {'åŠ¨æ€æ¸²æŸ“' if page.method == 'dynamic' else 'é™æ€çˆ¬å–'}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # ä¸»ä½“å†…å®¹
        lines.append(content)
        lines.append("")
        lines.append("---")
        lines.append("")

        # é¡µè„š
        lines.append("*æœ¬æ–‡ç”± Creeper è‡ªåŠ¨çˆ¬å–å¹¶æ¸…æ´—*")

        return '\n'.join(lines)

    async def save_async(self, item: URLItem, page: WebPage) -> Optional[Path]:
        """
        å¼‚æ­¥ä¿å­˜ç½‘é¡µå†…å®¹åˆ° Markdown æ–‡ä»¶

        Args:
            item: URL é¡¹ç›®
            page: ç½‘é¡µæ•°æ®

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            # åˆ›å»ºç›®å½•ç»“æ„
            h1_dir = self.output_dir / sanitize_filename(item.h1)
            h2_dir = h1_dir / sanitize_filename(item.h2)

            ensure_dir(h2_dir)

            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{sanitize_filename(item.h2)}.md"
            file_path = h2_dir / filename

            # å¼‚æ­¥ç”Ÿæˆ Markdown å†…å®¹
            markdown_content = await self._generate_markdown_async(item, page, h2_dir)

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_files'] += 1
            self.stats['total_size'] += file_path.stat().st_size

            logger.info(f"âœ“ æ–‡ä»¶å·²ä¿å­˜: {file_path.relative_to(self.output_dir)}")
            return file_path

        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_failed_urls(self, failed_items: list) -> Optional[Path]:
        """
        ä¿å­˜å¤±è´¥çš„ URL åˆ—è¡¨

        Args:
            failed_items: å¤±è´¥çš„ (URLItem, error_message) åˆ—è¡¨

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not config.SAVE_FAILED_URLS or not failed_items:
            return None

        try:
            filename = f"failed_urls_{get_timestamp().replace(':', '-').replace(' ', '_')}.txt"
            file_path = self.output_dir / filename

            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"# çˆ¬å–å¤±è´¥çš„ URL åˆ—è¡¨\n")
                f.write(f"# ç”Ÿæˆæ—¶é—´: {get_timestamp()}\n")
                f.write(f"# æ€»è®¡: {len(failed_items)} ä¸ª\n\n")

                for item, error in failed_items:
                    f.write(f"URL: {item.url}\n")
                    f.write(f"å±‚çº§: {item.h1} / {item.h2}\n")
                    f.write(f"é”™è¯¯: {error}\n")
                    f.write("-" * 80 + "\n\n")

            logger.info(f"âœ“ å¤±è´¥ URL åˆ—è¡¨å·²ä¿å­˜: {file_path}")
            return file_path

        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥ URL åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_stats(self) -> dict:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.output_dir.exists():
            return {'total_files': 0, 'total_dirs': 0}

        # ç»Ÿè®¡æ–‡ä»¶å’Œç›®å½•æ•°é‡
        md_files = list(self.output_dir.rglob('*.md'))
        dirs = [d for d in self.output_dir.rglob('*') if d.is_dir()]

        return {
            'total_files': len(md_files),
            'total_dirs': len(dirs),
            'output_dir': str(self.output_dir)
        }
