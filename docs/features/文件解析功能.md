# 文件解析功能 - 需求文档

> 生成时间：2025-11-28
> 基于项目：Creeper (v1.7.0)
> 技术栈：Python 3.8+ + AsyncIO + Redis + OpenAI API

---

## 项目概况

**技术栈**：Python 3.8+ + aiohttp（异步并发）+ Redis（去重缓存）+ OpenAI SDK（LLM 调用）+ argparse（CLI）
**架构模式**：模块化分层架构（CLI 层 → 业务逻辑层 → 数据层）
**代码风格**：snake_case 命名 / 中文注释 / Type Hints / 混合存储模式（Redis + 本地文件）

---

## 改动点

### 要实现什么
- **核心功能 1**：提供文件夹路径，扫描所有文件，独立调用 LLM 处理每个文件，生成一对一的输出文件
- **核心功能 2**：通过 Redis 缓存已处理文件的哈希，实现增量更新（仅处理新增/变更的文件）
- **核心功能 3**：支持自定义输出文件夹路径、提示词模板、文件扩展名过滤、强制重新处理等 CLI 参数

### 与现有功能的关系
- **复用模块**：
  - `file_aggregator.py:FileScanner` - 文件扫描和哈希计算（完全复用）
  - `file_aggregator.py:LLMAggregator` - LLM API 调用逻辑（需调整为单文件模式）
  - `prompt_templates.py:PromptTemplateManager` - 提示词模板加载（完全复用）
  - `dedup.py:DedupManager` - 混合存储模式的设计模式（参考实现）
- **集成位置**：新增独立 CLI 入口 `parser.py`（与 `aggregator.py` 同级）
- **差异对比**：

| 特性 | 文件整合（aggregator.py） | 文件解析（parser.py） |
|------|--------------------------|---------------------|
| 输入 | 文件夹（多个文件） | 文件夹（多个文件） |
| 输出 | 单个整合文件 | 多个文件（一对一） |
| 缓存粒度 | 文件夹级别（基于输出文件哈希） | 文件级别（基于输入文件哈希） |
| LLM 调用 | 批量整合所有文件 | 独立处理每个文件 |
| Redis Key | `creeper:aggregator:<output_hash>:files` | `creeper:parser:<file_hash>` |

### 新增依赖（如有）
- 无新依赖，完全基于现有技术栈

---

## 实现方案

### 需要修改的文件
```
src/file_aggregator.py  # 修改内容：抽取 LLMAggregator 为支持单文件模式
src/config.py           # 修改内容：新增 PARSER_* 配置项（可选，或直接复用 AGGREGATOR_* 配置）
```

### 需要新增的文件
```
src/file_parser.py      # 用途：文件解析器核心逻辑（ParserCache, FileParser）
parser.py               # 用途：CLI 入口（与 aggregator.py 同级）
tests/file_parser/      # 用途：测试文件，统一存放在此
tests/file_parser/test_parser_cache.py    # 测试缓存管理器
tests/file_parser/test_file_parser.py     # 测试解析器
prompts/file_parser.txt # 用途：默认提示词模板（可选，用户可自定义）
data/parser_cache.json  # 用途：本地持久化缓存文件（运行时自动生成）
```

### 实现步骤

**步骤 1：环境准备**
- [x] 创建新分支 `feature/file-parser`
- [ ] 创建新目录和文件骨架
  - `src/file_parser.py`
  - `parser.py`
  - `tests/file_parser/`
  - `prompts/file_parser.txt`

**步骤 2：核心实现**
- [ ] 实现 `ParserCache` 类（参考 `dedup.py` 的混合存储模式）
  - Redis Key 格式: `creeper:parser:<md5(file_path)>`
  - 存储内容: `{"path": str, "hash": str, "parsed_at": timestamp, "output_path": str}`
  - 本地文件: `data/parser_cache.json`
  - 方法:
    - `is_processed(file_item: FileItem, output_folder: str) -> bool`
    - `mark_processed(file_item: FileItem, output_path: str) -> None`
    - `get_unprocessed_files(files: List[FileItem], output_folder: str) -> List[FileItem]`
    - `_restore_from_file_if_needed() -> None`（启动时恢复）
    - `_save_to_file() -> None`（持久化到本地）

- [ ] 实现 `FileParser` 类（参考 `LLMAggregator`，调整为单文件模式）
  - 方法:
    - `async parse_file(file_item: FileItem, prompt_template: str) -> str`
      - 构造消息: System (模板) + User (单个文件内容)
      - 调用 OpenAI API
      - 返回解析结果
    - `async parse_directory(input_folder: str, output_folder: str, template: str, extensions: List[str], force: bool) -> None`
      - 扫描文件（复用 `FileScanner.scan_directory()`）
      - 过滤已处理文件（调用 `ParserCache.get_unprocessed_files()`）
      - 并发处理文件（`asyncio.gather(*tasks)`）
      - 保存结果到输出文件夹（保持相对路径结构）
      - 更新缓存（调用 `ParserCache.mark_processed()`）

- [ ] 添加错误处理和输入验证
  - 文件夹不存在 → 退出并提示
  - 输出文件夹不存在 → 自动创建
  - 模板文件不存在 → 提示可用模板
  - LLM API 失败 → 记录错误，继续处理其他文件
  - Redis 失败 → 降级为处理所有文件（记录警告）

**步骤 3：集成到现有系统**
- [ ] 创建 `parser.py` CLI 入口（参考 `aggregator.py`）
  - 参数设计:
    ```bash
    parser.py --input-folder <path> \
              --output-folder <path> \
              --template <name> \
              [--extensions .py,.md] \
              [--force] \
              [--debug] \
              [--concurrency 5] \
              [--list-templates]
    ```
  - 参数验证:
    - `--input-folder` 必填
    - `--output-folder` 必填
    - `--template` 必填
    - `--extensions` 默认: `.py,.md,.txt`
    - `--concurrency` 默认: 5（复用 `config.CONCURRENCY`）

- [ ] 创建默认提示词模板 `prompts/file_parser.txt`（可选）
  - 内容参考: "你是资深文档分析专家，请按照以下要求解析文件内容..."
  - 输出格式: Markdown

- [ ] 更新 `src/config.py`（可选，或直接复用 `AGGREGATOR_*` 配置）
  - 新增配置项:
    ```python
    PARSER_CACHE_FILE = os.path.join(DATA_DIR, "parser_cache.json")
    PARSER_REDIS_KEY_PREFIX = f"{REDIS_KEY_PREFIX}parser:"
    ```

**步骤 4：测试**
- [ ] 测试文件扫描和哈希计算（复用 `FileScanner`）
- [ ] 测试缓存管理器
  - 新文件检测
  - 文件变更检测（哈希变化）
  - Redis 恢复机制
  - 本地文件持久化
- [ ] 测试 LLM 解析
  - 单文件解析
  - 批量解析（并发）
  - API 失败降级
- [ ] 测试输出文件结构
  - 保持输入文件夹的相对路径
  - 输出文件扩展名保持为 `.md`
- [ ] **回归测试现有功能**（必须！）
  - 运行 `aggregator.py` 确保未破坏文件整合功能
  - 运行 `creeper.py` 确保爬虫功能正常
  - 运行所有现有测试 `pytest tests/`

**步骤 5：文档更新**
- [ ] 更新 `CHANGELOG.md`（见下方更新指南）
- [ ] 检查并更新 `CLAUDE.md` 文件
  - 新增"文件解析功能"章节
  - 更新"开发命令"章节（添加 `parser.py` 使用示例）
  - 更新"架构设计"章节（新增 `file_parser.py` 模块说明）
- [ ] 判断是否需要更新 `README.md`（见下方判断标准）
  - ✅ 需要更新：新增了用户可见的命令 `parser.py`
  - 更新内容：在"功能特性"或"使用方法"章节添加一句话说明

**步骤 6：提交代码**
- [ ] 使用 git 提交新增需求的实现
  ```bash
  git add .
  git commit -m "feat: 文件解析功能 - 支持文件夹批量解析和增量更新"
  ```

---

## 使用方式

### 命令行
```bash
# 基础用法：解析文件夹中的所有 Python 和 Markdown 文件
python parser.py --input-folder ./src \
                 --output-folder ./output/parsed \
                 --template code_analysis

# 自定义文件扩展名
python parser.py --input-folder ./docs \
                 --output-folder ./output/parsed_docs \
                 --template doc_summary \
                 --extensions .md,.txt,.rst

# 强制重新处理所有文件（忽略缓存）
python parser.py --input-folder ./src \
                 --output-folder ./output/parsed \
                 --template code_analysis \
                 --force

# 调试模式（显示详细日志）
python parser.py --input-folder ./src \
                 --output-folder ./output/parsed \
                 --template code_analysis \
                 --debug

# 自定义并发数
python parser.py --input-folder ./src \
                 --output-folder ./output/parsed \
                 --template code_analysis \
                 --concurrency 10

# 列出可用的提示词模板
python parser.py --list-templates
```

### 输出文件结构
保持输入文件夹的相对路径结构，扩展名统一为 `.md`：

```
输入文件夹：
./src/
├── parser.py
├── fetcher.py
└── utils/
    └── helpers.py

输出文件夹：
./output/parsed/
├── parser.md
├── fetcher.md
└── utils/
    └── helpers.md
```

### 配置项
复用现有的 `AGGREGATOR_*` 配置项（`.env` 文件）：

```bash
# LLM API 配置
AGGREGATOR_API_KEY=your_deepseek_api_key
AGGREGATOR_BASE_URL=https://api.deepseek.com
AGGREGATOR_MODEL=deepseek-chat
AGGREGATOR_MAX_TOKENS=8000
AGGREGATOR_TEMPERATURE=0.1

# Redis 配置（用于缓存）
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=1
ENABLE_LOCAL_PERSISTENCE=true  # 启用本地持久化

# 并发控制
CONCURRENCY=5  # 同时处理的文件数
```

---

## 完成检查清单

**代码质量**：
- [ ] 遵循项目代码风格（snake_case, 中文注释, Type Hints）
- [ ] 添加必要注释（核心逻辑和复杂算法）
- [ ] 错误处理完善（Redis 失败降级、LLM API 失败重试、文件读取失败跳过）
- [ ] 无安全漏洞（路径遍历防护、SSRF 防护）

**测试**：
- [ ] 新功能测试通过（缓存管理、文件解析、并发处理）
- [ ] 现有功能无影响（aggregator.py、creeper.py、所有测试）
- [ ] 边界条件处理（空文件夹、文件过大、API 失败、Redis 不可用）

**文档**：
- [ ] README 已判断并按需更新（新增命令，需要简单说明）
- [ ] CHANGELOG 已更新（见下方更新指南）
- [ ] CLAUDE.md 已检查并更新（新增模块、命令、架构说明）
- [ ] 提示词模板文档已创建（`prompts/file_parser.txt`）

---

## CHANGELOG.md 更新指南

**版本号规则**：新增功能（向后兼容）→ MINOR (1.8.0)

**模板**：
```markdown
## [1.8.0] - 2025-11-28

### Added
- **文件解析功能**：支持文件夹批量解析，每个文件独立调用 LLM 处理并生成对应的输出文件
  - 通过 Redis 实现增量更新，仅处理新增或变更的文件
  - 支持自定义输出文件夹、提示词模板、文件扩展名过滤
  - 保持输入文件夹的相对路径结构
  - 相关文件：`src/file_parser.py`, `parser.py`
  - 新增命令：`python parser.py --input-folder <path> --output-folder <path> --template <name>`
  - 新增缓存文件：`data/parser_cache.json`（本地持久化）

### Technical Details
- 复用 `FileScanner` 进行文件扫描和哈希计算
- 采用混合存储模式（Redis + 本地 JSON），防止缓存丢失
- 支持异步并发处理，默认并发数 5（可通过 `--concurrency` 调整）
```

**README.md 更新判断标准**：

**✅ 需要更新**：
- 新增了用户可见的命令 `parser.py`
- 这是核心功能，影响用户使用方式

**更新建议**（在 README.md 的"功能特性"或"使用方法"章节添加）：
```markdown
### 文件解析（v1.8 新增）
```bash
# 批量解析文件夹中的文件，每个文件独立调用 LLM 处理
python parser.py --input-folder ./src \
                 --output-folder ./output/parsed \
                 --template code_analysis
```
详细说明见 `docs/features/文件解析功能.md`
```

**CLAUDE.md 文件更新判断**：

**✅ 需要更新**：
- 新增了核心模块 `file_parser.py`
- 新增了 CLI 命令 `parser.py`
- 引入了新的缓存机制（文件级别缓存）

**更新建议**：
1. 在"开发命令"章节新增"运行文件解析"小节
2. 在"架构设计 → 核心组件"章节新增"文件解析器"说明
3. 在"项目结构约定"章节添加 `data/parser_cache.json` 说明

---

## 提交代码

在需求实现完成后，按以下顺序操作：

```bash
# 1. 创建并切换到新分支
git checkout -b feature/file-parser

# 2. 判断并更新 README.md（见上方判断标准）
# 3. 检查并更新 CLAUDE.md 文件（见上方判断标准）

# 4. 添加所有修改的文件
git add .

# 5. 提交，commit 信息使用需求文档的标题
git commit -m "feat: 文件解析功能 - 支持文件夹批量解析和增量更新"

# 6. 推送到远程仓库（如需要）
git push origin feature/file-parser
```

注意：
- **按顺序：先判断并更新 README.md → 检查并更新 CLAUDE.md 文件 → 提交代码**
- 使用 `feat:` 前缀表示这是新增功能
- 如更新了 README.md 或 CLAUDE.md，在 commit message 中可补充说明

---

## 注意事项

**技术风险**：
- ✅ 无新依赖，完全基于现有技术栈，无兼容性风险
- ⚠️ 并发处理大量文件时可能触发 LLM API 速率限制 → 添加重试机制和错误处理
- ⚠️ 输出文件夹路径可能包含 `..` 导致路径遍历 → 使用 `os.path.abspath()` 规范化路径

**兼容性**：
- ✅ 向后兼容，不影响现有 `aggregator.py` 和 `creeper.py` 功能
- ✅ 复用现有配置项，无需修改 `.env` 文件
- ✅ 新增独立 CLI 命令，不破坏现有 API

**性能考虑**：
- 默认并发数 5，避免 API 速率限制
- 大文件（>1MB）截断处理（复用 `AGGREGATOR_MAX_FILE_SIZE` 配置）
- Redis 缓存避免重复处理

**安全考虑**：
- 路径遍历防护：使用 `os.path.abspath()` 和 `os.path.commonpath()` 验证输出路径
- SSRF 防护：不涉及 URL 请求，无需额外防护
- 文件大小限制：复用 `AGGREGATOR_MAX_FILE_SIZE` 配置（默认 1MB）

---

## 实现细节补充

### Redis Key 设计
```python
# 格式: creeper:parser:<md5(file_path)>
# 示例: creeper:parser:5d41402abc4b2a76b9719d911017c592

# 存储内容（Hash 类型）:
{
    "path": "/absolute/path/to/file.py",
    "hash": "md5_hash_of_file_content",
    "parsed_at": "2025-11-28T12:00:00",
    "output_path": "/absolute/path/to/output/file.md"
}
```

### 增量更新逻辑
```python
def get_unprocessed_files(self, files: List[FileItem], output_folder: str) -> List[FileItem]:
    """获取未处理或已变更的文件"""
    unprocessed = []
    for file_item in files:
        key = f"{self.key_prefix}{hashlib.md5(file_item.path.encode()).hexdigest()}"
        cached_info = self.redis.hgetall(key)

        # 未处理 或 内容变更（哈希不同）
        if not cached_info or cached_info.get("hash") != file_item.hash:
            unprocessed.append(file_item)

    return unprocessed
```

### 输出文件路径映射
```python
def get_output_path(input_folder: str, input_file: str, output_folder: str) -> str:
    """保持相对路径结构，扩展名改为 .md"""
    rel_path = os.path.relpath(input_file, input_folder)
    output_path = os.path.join(output_folder, rel_path)
    output_path = os.path.splitext(output_path)[0] + ".md"
    return output_path

# 示例:
# input_folder: ./src
# input_file: ./src/utils/helpers.py
# output_folder: ./output/parsed
# 结果: ./output/parsed/utils/helpers.md
```

### 错误处理策略
```python
async def _process_file(self, file_item: FileItem, output_folder: str, template: str):
    """处理单个文件（带错误处理）"""
    try:
        # 解析文件
        result = await self.parse_file(file_item, template)

        # 保存结果
        output_path = self.get_output_path(file_item.path, output_folder)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(result)

        # 更新缓存
        self.cache.mark_processed(file_item, output_path)

        logger.info(f"✓ 已处理: {file_item.path} → {output_path}")

    except Exception as e:
        logger.error(f"✗ 处理失败: {file_item.path} - {str(e)}", exc_info=config.DEBUG)
        # 不阻塞其他文件处理
```

---

## 预期效果

**用户体验**：
- 一行命令批量处理文件夹
- 增量更新，仅处理变更文件（节省 API 调用成本）
- 保持文件夹结构，便于对比和查找
- 清晰的进度提示和错误日志

**性能表现**：
- 100 个文件（每个 5KB），并发 5，预计耗时 ~10 分钟（假设 LLM API 每次调用 3 秒）
- Redis 缓存命中率 >90%（第二次运行时）
- 内存占用 <100MB（异步处理，不会一次性加载所有文件）

**可扩展性**：
- 支持自定义提示词模板（通过 `--template` 参数）
- 支持任意文件扩展名（通过 `--extensions` 参数）
- 可轻松扩展为支持其他 LLM API（修改 `FileParser` 类）

---

## 示例：使用场景

### 场景 1：代码库文档化
```bash
# 分析所有 Python 文件，生成架构文档
python parser.py --input-folder ./src \
                 --output-folder ./docs/code_analysis \
                 --template code_summary \
                 --extensions .py
```

### 场景 2：教程内容整理
```bash
# 解析所有 Markdown 教程，生成总结
python parser.py --input-folder ./tutorials \
                 --output-folder ./output/tutorial_summaries \
                 --template tutorial_summary \
                 --extensions .md
```

### 场景 3：增量更新
```bash
# 第一次运行：处理所有文件
python parser.py --input-folder ./src --output-folder ./docs/parsed --template code_analysis

# 修改了 src/parser.py 后，第二次运行：仅处理变更的文件
python parser.py --input-folder ./src --output-folder ./docs/parsed --template code_analysis
# 输出: ✓ 已跳过 98 个未变更文件，处理 2 个新增/变更文件
```

---

## 后续优化方向（可选）

1. **进度条显示**：使用 `tqdm` 显示处理进度
2. **批处理优化**：支持批量调用 LLM API（减少网络开销）
3. **错误重试机制**：API 失败时自动重试（指数退避）
4. **并发控制优化**：根据 API 速率限制自动调整并发数
5. **输出格式可配置**：支持 JSON、HTML 等格式（通过 `--format` 参数）
6. **增量更新策略优化**：支持基于时间戳的缓存过期（而非仅基于哈希）

---

**准备开始实施！** 🚀
